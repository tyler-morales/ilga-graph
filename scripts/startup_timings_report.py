#!/usr/bin/env python3
"""Analyze .startup_timings.csv and print a clean terminal report.

This repo's startup timing CSV has evolved over time:
  - v1 header (10 cols):
    timestamp,total_s,load_s,analytics_s,export_s,votes_s,members,bills,dev_mode,seed_mode
  - v2 rows (16 cols):
    timestamp,total_s,load_s,analytics_s,seating_s,export_s,votes_s,slips_s,zip_s,
    members,bills,votes,slips,zctas,dev_mode,seed_mode

Because the logger writes the header only when the file is first created, it's normal to see mixed
schemas in one file. This tool detects the row shape and normalizes to a single internal model.
"""

from __future__ import annotations

import argparse
import math
import statistics
import sys
from collections.abc import Iterable
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

# Ensure the project is importable (match other scripts/* pattern)
ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT / "src"))


V1_COLS = [
    "timestamp",
    "total_s",
    "load_s",
    "analytics_s",
    "export_s",
    "votes_s",
    "members",
    "bills",
    "dev_mode",
    "seed_mode",
]

V2_COLS = [
    "timestamp",
    "total_s",
    "load_s",
    "analytics_s",
    "seating_s",
    "export_s",
    "votes_s",
    "slips_s",
    "zip_s",
    "members",
    "bills",
    "votes",
    "slips",
    "zctas",
    "dev_mode",
    "seed_mode",
]

FLOAT_COLS = {
    "total_s",
    "load_s",
    "analytics_s",
    "seating_s",
    "export_s",
    "votes_s",
    "slips_s",
    "zip_s",
}
INT_COLS = {"members", "bills", "votes", "slips", "zctas"}
BOOL_COLS = {"dev_mode", "seed_mode"}


@dataclass(frozen=True)
class StartupTiming:
    timestamp: datetime
    total_s: float
    load_s: float
    analytics_s: float
    seating_s: float
    export_s: float
    votes_s: float
    slips_s: float
    zip_s: float
    members: int
    bills: int
    votes: int | None
    slips: int | None
    zctas: int | None
    dev_mode: bool
    seed_mode: bool
    schema_len: int

    @property
    def steps_sum_s(self) -> float:
        return (
            self.load_s
            + self.analytics_s
            + self.seating_s
            + self.export_s
            + self.votes_s
            + self.slips_s
            + self.zip_s
        )

    @property
    def overhead_s(self) -> float:
        return max(0.0, self.total_s - self.steps_sum_s)


def _parse_bool(s: str) -> bool:
    s = s.strip().lower()
    if s in {"1", "true", "t", "yes", "y"}:
        return True
    if s in {"0", "false", "f", "no", "n"}:
        return False
    raise ValueError(f"Invalid boolean: {s!r}")


def _safe_float(s: str) -> float:
    s = s.strip()
    if not s:
        return 0.0
    return float(s)


def _safe_int(s: str) -> int:
    s = s.strip()
    if not s:
        return 0
    return int(s)


def _quantile(sorted_vals: list[float], q: float) -> float:
    """Nearest-rank quantile. Expects sorted_vals sorted ascending."""
    if not sorted_vals:
        return float("nan")
    if q <= 0:
        return sorted_vals[0]
    if q >= 1:
        return sorted_vals[-1]
    # nearest-rank: ceil(q*n)
    n = len(sorted_vals)
    k = max(1, math.ceil(q * n))
    return sorted_vals[k - 1]


def _fmt_s(x: float) -> str:
    if math.isnan(x):
        return "—"
    return f"{x:,.2f}s"


def _fmt_i(x: int | None) -> str:
    if x is None:
        return "—"
    return f"{x:,}"


def _pearson_r(xs: list[float], ys: list[float]) -> float:
    if len(xs) != len(ys) or len(xs) < 2:
        return float("nan")
    mx = statistics.fmean(xs)
    my = statistics.fmean(ys)
    num = 0.0
    dx2 = 0.0
    dy2 = 0.0
    for x, y in zip(xs, ys):
        dx = x - mx
        dy = y - my
        num += dx * dy
        dx2 += dx * dx
        dy2 += dy * dy
    den = math.sqrt(dx2) * math.sqrt(dy2)
    return float("nan") if den == 0 else (num / den)


def _split_csv_line(line: str) -> list[str]:
    # This file is generated by our own logger; values are simple and unquoted.
    return [part.strip() for part in line.rstrip("\n").split(",")]


def read_startup_timings(path: Path) -> list[StartupTiming]:
    if not path.exists():
        raise FileNotFoundError(str(path))

    rows: list[StartupTiming] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        if not raw.strip():
            continue
        parts = _split_csv_line(raw)
        if not parts:
            continue
        if parts[0] == "timestamp":
            continue

        schema_len = len(parts)
        if schema_len == len(V1_COLS):
            cols = V1_COLS
        elif schema_len == len(V2_COLS):
            cols = V2_COLS
        else:
            # Unknown row shape; skip, but do so loudly upstream by counting schema lens.
            # For now, ignore this row.
            continue

        # Avoid zip(..., strict=True) to keep this script runnable even on older Python.
        if len(cols) != len(parts):
            continue
        record: dict[str, Any] = dict(zip(cols, parts))

        ts = datetime.fromisoformat(record["timestamp"])

        # Normalize to v2 internal model.
        def f(name: str) -> float:
            return _safe_float(record.get(name, "0"))

        def i(name: str) -> int:
            return _safe_int(record.get(name, "0"))

        def bi(name: str) -> int | None:
            if name not in record:
                return None
            return _safe_int(record[name])

        def b(name: str) -> bool:
            return _parse_bool(record.get(name, "False"))

        rows.append(
            StartupTiming(
                timestamp=ts,
                total_s=f("total_s"),
                load_s=f("load_s"),
                analytics_s=f("analytics_s"),
                seating_s=f("seating_s"),
                export_s=f("export_s"),
                votes_s=f("votes_s"),
                slips_s=f("slips_s"),
                zip_s=f("zip_s"),
                members=i("members"),
                bills=i("bills"),
                votes=bi("votes"),
                slips=bi("slips"),
                zctas=bi("zctas"),
                dev_mode=b("dev_mode"),
                seed_mode=b("seed_mode"),
                schema_len=schema_len,
            )
        )

    rows.sort(key=lambda r: r.timestamp)
    return rows


def _window(items: list[StartupTiming], start: int, end: int) -> list[StartupTiming]:
    start = max(0, start)
    end = min(len(items), end)
    if start >= end:
        return []
    return items[start:end]


def _summary_stats(vals: Iterable[float]) -> dict[str, float]:
    vs = [v for v in vals if v is not None and not math.isnan(v)]
    vs.sort()
    if not vs:
        return {"p50": float("nan"), "p90": float("nan"), "mean": float("nan")}
    return {"p50": _quantile(vs, 0.5), "p90": _quantile(vs, 0.9), "mean": statistics.fmean(vs)}


def _build_report(
    timings: list[StartupTiming],
    recent_n: int,
    baseline_n: int,
    show_top: int,
) -> dict[str, Any]:
    if not timings:
        return {"timings": [], "recent": [], "baseline": [], "schema_counts": {}}

    schema_counts: dict[int, int] = {}
    for t in timings:
        schema_counts[t.schema_len] = schema_counts.get(t.schema_len, 0) + 1

    recent = timings[-recent_n:] if recent_n > 0 else timings[:]
    baseline = []
    if baseline_n > 0:
        baseline = _window(
            timings,
            max(0, len(timings) - recent_n - baseline_n),
            len(timings) - recent_n,
        )

    def metric_series(metric: str, items: list[StartupTiming]) -> list[float]:
        out: list[float] = []
        for t in items:
            out.append(getattr(t, metric))
        return out

    metrics = [
        "total_s",
        "load_s",
        "analytics_s",
        "seating_s",
        "export_s",
        "votes_s",
        "slips_s",
        "zip_s",
        "overhead_s",
    ]

    stats: dict[str, dict[str, dict[str, float]]] = {"recent": {}, "baseline": {}}
    for m in metrics:
        stats["recent"][m] = _summary_stats(metric_series(m, recent))
        stats["baseline"][m] = (
            _summary_stats(metric_series(m, baseline))
            if baseline
            else {"p50": float("nan"), "p90": float("nan"), "mean": float("nan")}
        )

    top = sorted(timings, key=lambda t: t.total_s, reverse=True)[: max(0, show_top)]

    # Simple "driver" correlations (only when counts exist).
    def corr_with_count(count_attr: str) -> float:
        xs: list[float] = []
        ys: list[float] = []
        for t in timings:
            c = getattr(t, count_attr)
            if c is None:
                continue
            xs.append(float(c))
            ys.append(t.total_s)
        return _pearson_r(xs, ys)

    drivers = {
        "bills": corr_with_count("bills"),
        "votes": corr_with_count("votes"),
        "slips": corr_with_count("slips"),
        "zctas": corr_with_count("zctas"),
    }

    return {
        "timings": timings,
        "recent": recent,
        "baseline": baseline,
        "schema_counts": schema_counts,
        "stats": stats,
        "top": top,
        "drivers": drivers,
    }


def _render(report: dict[str, Any]) -> None:
    try:
        from rich import box
        from rich.align import Align
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table
        from rich.text import Text
    except Exception:
        # Minimal non-rich fallback.
        timings: list[StartupTiming] = report.get("timings", [])
        if not timings:
            print("No timings found.")
            return
        last = timings[-1]
        print(
            "Runs: "
            f"{len(timings)}  Range: {timings[0].timestamp.isoformat()} "
            f"→ {last.timestamp.isoformat()}"
        )
        print(
            f"Last total: {last.total_s:.2f}s  load={last.load_s:.2f}s export={last.export_s:.2f}s"
        )
        return

    console = Console()

    timings: list[StartupTiming] = report.get("timings", [])
    if not timings:
        console.print("[bold red]No timings found.[/bold red]")
        return

    first = timings[0]
    last = timings[-1]
    schema_counts: dict[int, int] = report.get("schema_counts", {})

    header = Text()
    header.append("Startup Timings Report", style="bold cyan")
    header.append("\n")
    header.append(f"{len(timings)} runs  ", style="bold")
    header.append(f"{first.timestamp.isoformat()} → {last.timestamp.isoformat()}", style="dim")
    has_single_non_v2_schema = len(schema_counts) == 1 and list(schema_counts.keys())[0] != len(
        V2_COLS
    )
    if len(schema_counts) > 1 or has_single_non_v2_schema:
        header.append("\n")
        header.append("Schemas: ", style="bold yellow")
        schema_text = ", ".join(f"{k} cols × {v}" for k, v in sorted(schema_counts.items()))
        header.append(schema_text, style="yellow")
        header.append("\n")
        header.append(
            "Note: mixed schemas usually means the logger evolved but the old header stayed.",
            style="dim",
        )

    console.print(Panel(Align.left(header), border_style="cyan"))

    stats = report.get("stats", {})
    recent_stats = stats.get("recent", {})
    baseline_stats = stats.get("baseline", {})

    def row(metric: str, label: str) -> tuple[str, str, str, str, str, str]:
        r = recent_stats.get(metric, {})
        b = baseline_stats.get(metric, {})
        rp50 = r.get("p50", float("nan"))
        bp50 = b.get("p50", float("nan"))
        rp90 = r.get("p90", float("nan"))
        bp90 = b.get("p90", float("nan"))

        dp50 = rp50 - bp50 if (not math.isnan(rp50) and not math.isnan(bp50)) else float("nan")
        dp90 = rp90 - bp90 if (not math.isnan(rp90) and not math.isnan(bp90)) else float("nan")

        def style_delta(d: float) -> str:
            if math.isnan(d):
                return "dim"
            if d > 0.10:
                return "bold red"
            if d > 0.02:
                return "yellow"
            if d < -0.10:
                return "bold green"
            if d < -0.02:
                return "green"
            return "dim"

        return (
            label,
            _fmt_s(rp50),
            _fmt_s(bp50),
            (
                f"[{style_delta(dp50)}]{_fmt_s(dp50)}[/{style_delta(dp50)}]"
                if not math.isnan(dp50)
                else "—"
            ),
            _fmt_s(rp90),
            (
                f"[{style_delta(dp90)}]{_fmt_s(dp90)}[/{style_delta(dp90)}]"
                if not math.isnan(dp90)
                else "—"
            ),
        )

    table = Table(
        title="Recent vs Baseline (p50/p90)",
        box=box.SIMPLE_HEAVY,
        header_style="bold",
        show_lines=False,
    )
    table.add_column("Metric", style="bold")
    table.add_column("Recent p50", justify="right")
    table.add_column("Baseline p50", justify="right", style="dim")
    table.add_column("Δ p50", justify="right")
    table.add_column("Recent p90", justify="right")
    table.add_column("Δ p90", justify="right")

    table.add_row(*row("total_s", "Total"))
    table.add_row(*row("load_s", "Load"))
    table.add_row(*row("analytics_s", "Analytics"))
    table.add_row(*row("seating_s", "Seating"))
    table.add_row(*row("export_s", "Export"))
    table.add_row(*row("votes_s", "Votes"))
    table.add_row(*row("slips_s", "Slips"))
    table.add_row(*row("zip_s", "ZIP crosswalk"))
    table.add_row(*row("overhead_s", "Overhead"))

    console.print(table)

    top: list[StartupTiming] = report.get("top", [])
    if top:
        t2 = Table(title="Slowest Runs", box=box.SIMPLE, header_style="bold")
        t2.add_column("When", style="dim")
        t2.add_column("Total", justify="right")
        t2.add_column("Load", justify="right")
        t2.add_column("Export", justify="right")
        t2.add_column("ZIP", justify="right")
        t2.add_column("Bills", justify="right")
        t2.add_column("Votes", justify="right")
        t2.add_column("Slips", justify="right")
        for t in top:
            t2.add_row(
                t.timestamp.isoformat(timespec="seconds"),
                _fmt_s(t.total_s),
                _fmt_s(t.load_s),
                _fmt_s(t.export_s),
                _fmt_s(t.zip_s),
                _fmt_i(t.bills),
                _fmt_i(t.votes),
                _fmt_i(t.slips),
            )
        console.print(t2)

    drivers = report.get("drivers", {})
    dpanel = Text()
    dpanel.append("Correlation with total_s (Pearson r)\n", style="bold")
    for k in ["bills", "votes", "slips", "zctas"]:
        r = drivers.get(k, float("nan"))
        if math.isnan(r):
            dpanel.append(f"- {k:<5}: —\n", style="dim")
        else:
            style = "green" if r < -0.3 else ("red" if r > 0.3 else "yellow")
            dpanel.append(f"- {k:<5}: {r:+.2f}\n", style=style)
    dpanel.append(
        "\nTip: big step-changes in bills/votes/slips counts usually explain load/export growth.",
        style="dim",
    )
    console.print(Panel(dpanel, border_style="dim"))


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Analyze .startup_timings.csv and print a regression report."
    )
    parser.add_argument(
        "--file",
        type=str,
        default=".startup_timings.csv",
        help="Path to the startup timings CSV (default: .startup_timings.csv)",
    )
    parser.add_argument(
        "--recent",
        type=int,
        default=25,
        help="How many most-recent runs to summarize (default: 25)",
    )
    parser.add_argument(
        "--baseline",
        type=int,
        default=25,
        help="How many runs before 'recent' to use as baseline (default: 25)",
    )
    parser.add_argument(
        "--top",
        type=int,
        default=8,
        help="Show N slowest runs (default: 8)",
    )
    args = parser.parse_args()

    path = Path(args.file)

    try:
        from rich.console import Console
        from rich.progress import Progress, SpinnerColumn, TextColumn
    except Exception:
        timings = read_startup_timings(path)
        report = _build_report(
            timings,
            recent_n=args.recent,
            baseline_n=args.baseline,
            show_top=args.top,
        )
        _render(report)
        return

    console = Console()
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        transient=True,
        console=console,
    ) as progress:
        task = progress.add_task("Reading CSV", total=None)
        timings = read_startup_timings(path)
        progress.update(task, description="Computing stats")
        report = _build_report(
            timings,
            recent_n=args.recent,
            baseline_n=args.baseline,
            show_top=args.top,
        )
        progress.update(task, description="Rendering report")

    _render(report)


if __name__ == "__main__":
    main()
